{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# read in RecipeNLG Dataset\n",
    "\n",
    "df = pd.read_csv('RecipeNLG_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeout relevant columns\n",
    "# convert from string to list literal\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "df_cols = df[[\"ingredients\",\"NER\",\"directions\"]]\n",
    "\n",
    "data = df_cols.sample(n = 10000, random_state = 42)\n",
    "\n",
    "data[\"ingredients\"] = data[\"ingredients\"].apply(literal_eval)  \n",
    "data[\"directions\"] = data[\"directions\"].apply(literal_eval)\n",
    "data[\"NER\"] = data[\"NER\"].apply(literal_eval)\n",
    "data.reset_index(inplace=True, drop = True)\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randymi\\AppData\\Local\\Temp\\ipykernel_19752\\821735151.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_ingredients[\"ingredients\"] = data_ingredients[\"ingredients\"].apply(ingredients_to_string)\n"
     ]
    }
   ],
   "source": [
    "# data ingredients\n",
    "import numpy as np\n",
    "data_ingredients = data[[\"ingredients\",\"NER\"]]\n",
    "\n",
    "def ingredients_to_string(x : list):\n",
    "    return \", \".join(x)\n",
    "\n",
    "def get_NER_labels(row):\n",
    "    ingredients_sentence = row[\"ingredients\"]\n",
    "\n",
    "    sentence_indexed = np.array([0] * len(ingredients_sentence))\n",
    "\n",
    "    NER = row[\"NER\"]\n",
    "    entity_list = []\n",
    "    doc = nlp.make_doc(ingredients_sentence)\n",
    "    for food in NER:\n",
    "        try:\n",
    "            food_start_index = ingredients_sentence.index(food)\n",
    "            entity_list.append((food_start_index, food_start_index + len(food), \"FOOD\"))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        return Example.from_dict(doc, {\"entities\": entity_list})\n",
    "    except ValueError:\n",
    "        return Example.from_dict(doc, {\"entities\": []})\n",
    "\n",
    "data_ingredients[\"ingredients\"] = data_ingredients[\"ingredients\"].apply(ingredients_to_string)\n",
    "\n",
    "ingredients_NER_labels = data_ingredients.apply(get_NER_labels, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\randymi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def remove_descriptor_nltk(food_name):\n",
    "    words = nltk.word_tokenize(food_name)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    result_words = []\n",
    "    for word, tag in tagged_words:\n",
    "        if tag not in ['JJ', 'JJR', 'JJS']:\n",
    "            result_words.append(word)\n",
    "    return \" \".join(result_words)\n",
    "    \n",
    "wnl.lemmatize(remove_descriptor_nltk(\"green onions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randymi\\AppData\\Local\\Temp\\ipykernel_19752\\1479435369.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_directions[\"directions\"] = data_directions[\"directions\"].apply(directions_to_string)\n"
     ]
    }
   ],
   "source": [
    "# data directions\n",
    "\n",
    "data_directions = data[[\"directions\",\"NER\"]]\n",
    "\n",
    "def directions_to_string(x : list):\n",
    "    return \" \".join(x)\n",
    "\n",
    "def get_NER_labels(row):\n",
    "    ingredients_sentence = row[\"directions\"]\n",
    "\n",
    "    sentence_indexed = np.array([0] * len(ingredients_sentence))\n",
    "\n",
    "    NER = row[\"NER\"]\n",
    "    entity_list = []\n",
    "    doc = nlp.make_doc(ingredients_sentence)\n",
    "    for food in NER:\n",
    "        try:\n",
    "            food_start_index = ingredients_sentence.index(food)\n",
    "            entity_list.append((food_start_index, food_start_index + len(food), \"FOOD\"))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        return Example.from_dict(doc, {\"entities\": entity_list})\n",
    "    except ValueError:\n",
    "        return Example.from_dict(doc, {\"entities\": []})\n",
    "\n",
    "data_directions[\"directions\"] = data_directions[\"directions\"].apply(directions_to_string)\n",
    "\n",
    "directions_NER_labels = data_directions.apply(get_NER_labels, axis = 1)\n",
    "\n",
    "# filter out examples with no NER labels\n",
    "directions_NER_labels = directions_NER_labels[directions_NER_labels.apply(lambda x: len(set(x.get_aligned_ner())) != 1)]\n",
    "\n",
    "directions_NER_labels.reset_index(inplace=True, drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pipeline component and adding food label\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"FOOD\")\n",
    "\n",
    "def make_training_set(ingredients_proportion, directions_proportion, count = 1000):\n",
    "    ingredients = ingredients_NER_labels.sample(n = int(count * ingredients_proportion))\n",
    "    directions = directions_NER_labels.sample(n = int(count * directions_proportion))\n",
    "    return pd.concat([ingredients, directions]).reset_index(drop = True)\n",
    "\n",
    "# 80/20\n",
    "training_set1 = make_training_set(0.8, 0.2, count = 1000)\n",
    "\n",
    "# 60/40\n",
    "training_set2 = make_training_set(0.6, 0.4, count = 1000)\n",
    "\n",
    "# 50/50\n",
    "training_set3 = make_training_set(0.5, 0.5, count = 1000)\n",
    "\n",
    "# 40/60\n",
    "training_set4 = make_training_set(0.4, 0.6, count = 1000)\n",
    "\n",
    "# 20/80\n",
    "training_set5 = make_training_set(0.2, 0.8, count = 1000)\n",
    "\n",
    "TRAIN_DATA = ingredients_NER_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 131.29200112516946}\n",
      "Losses {'ner': 325.4222207302097}\n",
      "Losses {'ner': 505.1897349367963}\n",
      "Losses {'ner': 699.9145621813722}\n",
      "Losses {'ner': 924.5307927122096}\n",
      "Losses {'ner': 1189.5927112439217}\n",
      "Losses {'ner': 1406.0198667996774}\n",
      "Losses {'ner': 1625.1059985512147}\n",
      "Losses {'ner': 1847.4772617735125}\n",
      "Losses {'ner': 2078.770390550257}\n",
      "Losses {'ner': 2332.1886197018275}\n",
      "Losses {'ner': 2620.8461362872868}\n",
      "Losses {'ner': 2907.2317710044845}\n",
      "Losses {'ner': 3154.7714282126326}\n",
      "Losses {'ner': 3465.7790194401878}\n",
      "Losses {'ner': 3741.3430011299656}\n",
      "Losses {'ner': 4143.440052929263}\n",
      "Losses {'ner': 4573.36303899277}\n",
      "Losses {'ner': 4985.639394730563}\n",
      "Losses {'ner': 5477.899669501465}\n",
      "Losses {'ner': 5986.8640260928805}\n",
      "Losses {'ner': 6458.709009510356}\n",
      "Losses {'ner': 7027.56669078477}\n",
      "Losses {'ner': 7575.832898193063}\n",
      "Losses {'ner': 8086.60852722257}\n",
      "Losses {'ner': 8640.882239585664}\n",
      "Losses {'ner': 9314.865394836213}\n",
      "Losses {'ner': 10082.106198562815}\n",
      "Losses {'ner': 10742.77647532387}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\randymi\\Desktop\\fridgechef\\food_ner\\main.ipynb Cell 7\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/randymi/Desktop/fridgechef/food_ner/main.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m batches \u001b[39m=\u001b[39m minibatch(TRAIN_DATA, size\u001b[39m=\u001b[39mcompounding(\u001b[39m32\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m1.06\u001b[39m))    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/randymi/Desktop/fridgechef/food_ner/main.ipynb#X31sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batches:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/randymi/Desktop/fridgechef/food_ner/main.ipynb#X31sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m   nlp\u001b[39m.\u001b[39;49mupdate(batch, losses\u001b[39m=\u001b[39;49mlosses, drop\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, sgd \u001b[39m=\u001b[39;49m optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/randymi/Desktop/fridgechef/food_ner/main.ipynb#X31sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLosses \u001b[39m\u001b[39m{\u001b[39;00mlosses\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/randymi/Desktop/fridgechef/food_ner/main.ipynb#X31sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00miteration\u001b[39m}\u001b[39;00m\u001b[39m Loses\u001b[39m\u001b[39m\"\u001b[39m, losses)\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\language.py:1155\u001b[0m, in \u001b[0;36mLanguage.update\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[39mfor\u001b[39;00m name, proc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline:\n\u001b[0;32m   1153\u001b[0m     \u001b[39m# ignore statements are used here because mypy ignores hasattr\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mupdate\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1155\u001b[0m         proc\u001b[39m.\u001b[39mupdate(examples, sgd\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, losses\u001b[39m=\u001b[39mlosses, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg[name])  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m     \u001b[39mif\u001b[39;00m sgd \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1157\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1158\u001b[0m             name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude\n\u001b[0;32m   1159\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(proc, ty\u001b[39m.\u001b[39mTrainableComponent)\n\u001b[0;32m   1160\u001b[0m             \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mis_trainable\n\u001b[0;32m   1161\u001b[0m             \u001b[39mand\u001b[39;00m proc\u001b[39m.\u001b[39mmodel \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1162\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:426\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\ml\\parser_model.pyx:297\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.finish_steps\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\thinc\\layers\\chain.py:61\u001b[0m, in \u001b[0;36mforward.<locals>.backprop\u001b[1;34m(dY)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: OutT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[0;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(callbacks):\n\u001b[1;32m---> 61\u001b[0m         dX \u001b[39m=\u001b[39m callback(dY)\n\u001b[0;32m     62\u001b[0m         dY \u001b[39m=\u001b[39m dX\n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m dX\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\thinc\\layers\\chain.py:61\u001b[0m, in \u001b[0;36mforward.<locals>.backprop\u001b[1;34m(dY)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: OutT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[0;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(callbacks):\n\u001b[1;32m---> 61\u001b[0m         dX \u001b[39m=\u001b[39m callback(dY)\n\u001b[0;32m     62\u001b[0m         dY \u001b[39m=\u001b[39m dX\n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m dX\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward.<locals>.backprop\u001b[1;34m(dYs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYs: ListXd) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ListXd:\n\u001b[0;32m     76\u001b[0m     dYf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten(dYs, pad\u001b[39m=\u001b[39mpad)\n\u001b[1;32m---> 77\u001b[0m     dXf \u001b[39m=\u001b[39m get_dXf(dYf)\n\u001b[0;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39munflatten(dXf, lengths, pad\u001b[39m=\u001b[39mpad)\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\thinc\\layers\\chain.py:61\u001b[0m, in \u001b[0;36mforward.<locals>.backprop\u001b[1;34m(dY)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: OutT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[0;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(callbacks):\n\u001b[1;32m---> 61\u001b[0m         dX \u001b[39m=\u001b[39m callback(dY)\n\u001b[0;32m     62\u001b[0m         dY \u001b[39m=\u001b[39m dX\n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m dX\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\thinc\\layers\\residual.py:30\u001b[0m, in \u001b[0;36mforward.<locals>.backprop\u001b[1;34m(d_output)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(d_output: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[1;32m---> 30\u001b[0m     dX \u001b[39m=\u001b[39m backprop_layer(d_output)\n\u001b[0;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(d_output, \u001b[39mlist\u001b[39m):\n\u001b[0;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [d_output[i] \u001b[39m+\u001b[39m dX[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(d_output))]\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\thinc\\layers\\chain.py:61\u001b[0m, in \u001b[0;36mforward.<locals>.backprop\u001b[1;34m(dY)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: OutT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[0;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(callbacks):\n\u001b[1;32m---> 61\u001b[0m         dX \u001b[39m=\u001b[39m callback(dY)\n\u001b[0;32m     62\u001b[0m         dY \u001b[39m=\u001b[39m dX\n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m dX\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\thinc\\layers\\chain.py:61\u001b[0m, in \u001b[0;36mforward.<locals>.backprop\u001b[1;34m(dY)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: OutT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[0;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(callbacks):\n\u001b[1;32m---> 61\u001b[0m         dX \u001b[39m=\u001b[39m callback(dY)\n\u001b[0;32m     62\u001b[0m         dY \u001b[39m=\u001b[39m dX\n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m dX\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\thinc\\layers\\chain.py:61\u001b[0m, in \u001b[0;36mforward.<locals>.backprop\u001b[1;34m(dY)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: OutT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[0;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(callbacks):\n\u001b[1;32m---> 61\u001b[0m         dX \u001b[39m=\u001b[39m callback(dY)\n\u001b[0;32m     62\u001b[0m         dY \u001b[39m=\u001b[39m dX\n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m dX\n",
      "File \u001b[1;32mc:\\Users\\randymi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\thinc\\layers\\maxout.py:65\u001b[0m, in \u001b[0;36mforward.<locals>.backprop\u001b[1;34m(d_best)\u001b[0m\n\u001b[0;32m     63\u001b[0m dW \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape3f(model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mgemm(dY, X, trans1\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), nO, nP, nI)\n\u001b[0;32m     64\u001b[0m model\u001b[39m.\u001b[39minc_grad(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m, dW)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mgemm(dY, model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mreshape2f(W, nO \u001b[39m*\u001b[39;49m nP, nI))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "optimizer = nlp.create_optimizer()\n",
    "optimizer.learn_rate = 0.01\n",
    "\n",
    "\n",
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "def compound(start, stop, factor):\n",
    "\n",
    "    # initialize counter\n",
    "    value = start\n",
    "    counter = 1\n",
    "\n",
    "    # loop until counter is less than n\n",
    "    while value < stop:\n",
    "\n",
    "        # produce the current value of the counter\n",
    "         \n",
    "        yield value\n",
    "        value = value * (factor ** counter)\n",
    "\n",
    "        # increment the counter\n",
    "        counter += 1\n",
    "      \n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 100 iterations\n",
    "  for iteration in range(100):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4, 32, 1.05))    \n",
    "\n",
    "    for batch in batches:\n",
    "      nlp.update(batch, losses=losses, drop=0.5, sgd = optimizer)\n",
    "      print(f\"Losses {losses}\")\n",
    "    \n",
    "    print(f\"Epoch {iteration} Loses\", losses)\n",
    "\n",
    "    # every 10 epochs, save the model\n",
    "    if iteration % 10 == 0:\n",
    "      nlp.to_disk(f'tmp/ingredients_model{iteration}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.4\n",
      "2.7439999999999993\n",
      "7.529535999999997\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# iterate over the generator object produced by my_generator\n",
    "for value in compound(1,10,1.4):\n",
    "\n",
    "    # print each value produced by generator\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('tmp/ingredients_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(chicken, rice, black beans)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = nlp(\"chicken, rice, and black beans\")\n",
    "d.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'O', 'O', 'O', 'U-FOOD', 'O', 'O', 'O', 'O', 'U-FOOD', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'O', 'O', 'U-FOOD', 'O', 'O', 'O', 'O', 'U-FOOD', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'O', 'U-FOOD'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['1', '1/2', 'lb', '.', 'ground', 'beef', ',', '1', '1/4', 'tsp', '.', 'onion', ',', '1', 'tsp', '.', 'parsley', ',', '1', 'can', 'mushroom', 'soup', ',', '1', 'c.', 'sour', 'cream', ',', '1/4', 'tsp', '.', 'garlic', 'powder', ',', '1', 'tsp', '.', 'salt', ',', '1/4', 'tsp', '.', 'pepper', ',', '1', 'can', 'tomato', 'soup', ',', '1/2', 'c.', 'milk'], 'SPACY': [True, True, False, True, True, False, True, True, True, False, True, False, True, True, False, True, False, True, True, True, True, False, True, True, True, True, False, True, True, False, True, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True, True, False, True, True, True, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'U-FOOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-FOOD', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'U-FOOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'O', 'B-FOOD', 'I-FOOD', 'I-FOOD', 'I-FOOD', 'L-FOOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['1', 'bunch', 'broccoli', '(', 'aprx', '.', '1', '1/4', 'pounds', ')', ',', 'salt', ',', '1/4', 'cup', 'olive', 'oil', ',', '4', 'garlic', 'cloves', ',', 'thinly', 'sliced', ',', '1', 'pinch', 'crushed', 'red', 'pepper', 'flakes', '(', 'I', 'use', 'a', 'big', 'pinch', 'as', 'I', 'love', 'spicy', 'food', ')', ',', '8', 'ounces', 'rigatoni', 'pasta', ',', '1/2', 'cup', 'freshly', 'grated', 'pecorino', 'romano', 'cheese', 'or', '1/2', 'cup', 'parmigiano', '-', 'reggiano', 'cheese'], 'SPACY': [True, True, True, False, False, True, True, True, False, False, True, False, True, True, True, True, False, True, True, True, False, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, True, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'U-FOOD', 'O', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'O', 'U-FOOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FOOD', 'L-FOOD', 'O', 'O', 'O', 'O', 'U-FOOD', 'O', 'O', 'O', 'O', 'O', 'U-FOOD'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['1', 'lb', '.', 'margarine', ',', '1', 'lb', '.', 'Velveeta', 'cheese', ',', '1', 'c.', 'cocoa', ',', '4', '(', '1', 'lb', '.', ')', 'boxes', 'powdered', 'sugar', ',', '3', 'Tbsp', '.', 'vanilla', ',', '4', 'to', '6', 'c.', 'nuts'], 'SPACY': [True, False, True, False, True, True, False, True, True, False, True, True, True, False, True, True, False, True, False, False, True, True, True, False, True, True, False, True, False, True, True, True, True, True, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['2', 'thin', 'slices', 'reduced', '-', 'fat', 'Swiss', 'cheese', ',', '4', 'chicken', 'breast', 'halves', '(', '1/4', '-', 'inch', 'thick', ';', '4', 'oz', '.', 'each', ')', ',', '2', 'Tbsp', '.', 'all', '-', 'purpose', 'flour', ',', '1/2', 'tsp', '.', 'black', 'pepper', ',', '1', 'Tbsp', '.', 'unsalted', 'butter', 'or', 'margarine', ',', '1/2', 'c.', 'reduced', '-', 'sodium', 'chicken', 'broth', ',', '1/4', 'c.', 'reduced', '-', 'sodium', 'chicken', 'broth', ',', '1/4', 'tsp', '.', 'dried', 'oregano'], 'SPACY': [True, True, True, False, False, True, True, False, True, True, True, True, True, False, False, False, True, False, True, True, False, True, False, False, True, True, False, True, False, False, True, False, True, True, False, True, True, False, True, True, False, True, True, True, True, False, True, True, True, False, False, True, True, False, True, True, True, False, False, True, True, False, True, True, False, True, True, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001)).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "random.shuffle(TRAIN_DATA)\n",
    "losses = {}\n",
    "\n",
    "# batch up the examples using spaCy's minibatch\n",
    "# minibatch is generator which returns a random sample of train_data of size = n. Compounding is generator returning 4 * (1.001)^i while < 32    \n",
    "batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x17ec1d6d640>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1,2,3]]\n",
    "zip(*x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
